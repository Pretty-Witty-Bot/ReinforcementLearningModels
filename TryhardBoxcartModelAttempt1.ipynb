{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TryhardBoxcartModelAttempt1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pretty-Witty-Bot/ReinforcementLearningModels/blob/main/TryhardBoxcartModelAttempt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp6bEC0-684z",
        "outputId": "17536e1f-c80e-424d-85e6-83d3edfc2bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Install the necessary libraries and modules.\n",
        "\"\"\"\n",
        "\n",
        "!pip install gym\n",
        "!pip install gym[classic_control]\n",
        "!pip install torch\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Import the necessary libraries and modules.\n",
        "\"\"\"\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt \n",
        "from gym.wrappers import RecordVideo"
      ],
      "metadata": {
        "id": "FP0v31MM7rgf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize the environment and the video recorder.\n",
        "\"\"\"\n",
        "\n",
        "env = gym.make('MountainCar-v0', render_mode = 'rgb_array')\n",
        "# Store videos in ./naive_video directory.\n",
        "video_recorder = RecordVideo(env, './naive_video') "
      ],
      "metadata": {
        "id": "ALb9NkqX8HfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Figure out the dimensions that we are working with.\n",
        "\"\"\"\n",
        "\n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "\n",
        "print(\"The observation space: {}\".format(obs_space))\n",
        "print(\"The action space: {}\".format(action_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB1ayxcGM33o",
        "outputId": "e355063f-864c-4c92-b397-a7d92325fb4a"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "The action space: Discrete(3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "QNetwork Class. Derived from: https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda\n",
        "\"\"\"\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\" Actor (Policy) Model.\"\"\"\n",
        "    def __init__(self, state_size = 2, action_size = 3, seed = 0, fc1_unit=64,\n",
        "                 fc2_unit = 48, fc3_unit = 24):\n",
        "        \"\"\"\n",
        "        Initialize parameters and build model.\n",
        "        Params\n",
        "        =======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_unit (int): Number of nodes in first hidden layer\n",
        "            fc2_unit (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork,self).__init__() ## calls __init__ method of nn.Module class\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1= nn.Linear(state_size,fc1_unit)\n",
        "        self.fc2 = nn.Linear(fc1_unit,fc2_unit)\n",
        "        self.fc3 = nn.Linear(fc2_unit,fc3_unit)\n",
        "        self.fc4 = nn.Linear(fc3_unit,action_size)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # x = state\n",
        "        \"\"\"\n",
        "        Build a network that maps state -> action values.\n",
        "        \"\"\"\n",
        "        #print(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.fc4(x)"
      ],
      "metadata": {
        "id": "a9aowZAHR1k0"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Agent Class. Derived from: https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random \n",
        "from collections import namedtuple, deque \n",
        "\n",
        "##Importing the model (function approximator for Q-table)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "BUFFER_SIZE = int(1e5)  #replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-1              # for soft update of target parameters\n",
        "LR = 1e-1               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns form environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        =======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        \n",
        "        \n",
        "        #Q- Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
        "        \n",
        "        # Replay memory \n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE,BATCH_SIZE,seed)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "        \n",
        "    def step(self, state, action, reward, next_step, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_step, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step+1)% UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get radom subset and learn\n",
        "\n",
        "            if len(self.memory)>BATCH_SIZE:\n",
        "                experience = self.memory.sample()\n",
        "                self.learn(experience, GAMMA)\n",
        "    def act(self, state, eps = 0):\n",
        "        \"\"\"Returns action for given state as per current policy\n",
        "        Params\n",
        "        =======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "        #print(action_values)\n",
        "\n",
        "        #Epsilon -greedy action selction\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "            \n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\n",
        "        Params\n",
        "        =======\n",
        "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_state, dones = experiences\n",
        "        ## TODO: compute and minimize the loss\n",
        "        criterion = torch.nn.MSELoss()\n",
        "        # Local model is one which we need to train so it's in training mode\n",
        "        self.qnetwork_local.train()\n",
        "        # Target model is one with which we need to get our target so it's in evaluation mode\n",
        "        # So that when we do a forward pass with target model it does not calculate gradient.\n",
        "        # We will update target model weights with soft_update function\n",
        "        self.qnetwork_target.eval()\n",
        "        #shape of output from the model (batch_size,action_dim) = (64,4)\n",
        "        predicted_targets = self.qnetwork_local(states).gather(1,actions)\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            labels_next = self.qnetwork_target(next_state).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # .detach() ->  Returns a new Tensor, detached from the current graph.\n",
        "        labels = rewards + (gamma* labels_next*(1-dones))\n",
        "        \n",
        "        loss = criterion(predicted_targets,labels).to(device)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local,self.qnetwork_target,TAU)\n",
        "            \n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        =======\n",
        "            local model (PyTorch model): weights will be copied from\n",
        "            target model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(),\n",
        "                                           local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
        "            \n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed -size buffe to store experience tuples.\"\"\"\n",
        "    \n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        \n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
        "                                                               \"action\",\n",
        "                                                               \"reward\",\n",
        "                                                               \"next_state\",\n",
        "                                                               \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "        \n",
        "    def add(self,state, action, reward, next_state,done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experiences(state,action,reward,next_state,done)\n",
        "        self.memory.append(e)\n",
        "        \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
        "        experiences = random.sample(self.memory,k=self.batch_size)\n",
        "        \n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        \n",
        "        return (states,actions,rewards,next_states,dones)\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "8Y-mXfk7SCSF"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run the DQN Training Algorithm. Derived from: https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda\n",
        "\"\"\"\n",
        "\n",
        "agent = Agent(state_size=2,action_size=3,seed=0)\n",
        "\n",
        "def dqn(n_episodes= 200, max_t = 1000, eps_start=0.99):\n",
        "    \"\"\"Deep Q-Learning\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training epsiodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon \n",
        "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
        "        \n",
        "    \"\"\"\n",
        "    episode_scores = []\n",
        "    scores = [] # list containing score from each episode\n",
        "    eps = eps_start\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state,eps)\n",
        "            next_state,pre_reward,done,_ = env.step(action)\n",
        "            reward = -(0.5 - next_state[0]) + pre_reward\n",
        "            agent.step(state,action,reward,next_state,done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "            scores.append(score) ## sae the most recent score\n",
        "        print('\\rEpisode {}\\tAverage Score {:.2f}'.format(i_episode,np.mean(scores)))\n",
        "        episode_scores.append(np.mean(scores))\n",
        "    return scores, episode_scores\n",
        "\n",
        "scores, episode_scores = dqn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef4cz_VISN6S",
        "outputId": "e5e08029-52c7-4959-b468-9626e4bf6c05"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1\tAverage Score -201.51\n",
            "Episode 2\tAverage Score -201.71\n",
            "Episode 3\tAverage Score -201.96\n",
            "Episode 4\tAverage Score -202.21\n",
            "Episode 5\tAverage Score -202.24\n",
            "Episode 6\tAverage Score -202.40\n",
            "Episode 7\tAverage Score -202.31\n",
            "Episode 8\tAverage Score -202.12\n",
            "Episode 9\tAverage Score -202.16\n",
            "Episode 10\tAverage Score -202.13\n",
            "Episode 11\tAverage Score -202.18\n",
            "Episode 12\tAverage Score -202.23\n",
            "Episode 13\tAverage Score -202.27\n",
            "Episode 14\tAverage Score -202.32\n",
            "Episode 15\tAverage Score -202.18\n",
            "Episode 16\tAverage Score -202.13\n",
            "Episode 17\tAverage Score -202.20\n",
            "Episode 18\tAverage Score -202.23\n",
            "Episode 19\tAverage Score -202.18\n",
            "Episode 20\tAverage Score -202.19\n",
            "Episode 21\tAverage Score -202.23\n",
            "Episode 22\tAverage Score -202.24\n",
            "Episode 23\tAverage Score -202.29\n",
            "Episode 24\tAverage Score -202.25\n",
            "Episode 25\tAverage Score -202.28\n",
            "Episode 26\tAverage Score -202.30\n",
            "Episode 27\tAverage Score -202.31\n",
            "Episode 28\tAverage Score -202.37\n",
            "Episode 29\tAverage Score -202.40\n",
            "Episode 30\tAverage Score -202.41\n",
            "Episode 31\tAverage Score -202.36\n",
            "Episode 32\tAverage Score -202.37\n",
            "Episode 33\tAverage Score -202.38\n",
            "Episode 34\tAverage Score -202.41\n",
            "Episode 35\tAverage Score -202.38\n",
            "Episode 36\tAverage Score -202.41\n",
            "Episode 37\tAverage Score -202.38\n",
            "Episode 38\tAverage Score -202.37\n",
            "Episode 39\tAverage Score -202.36\n",
            "Episode 40\tAverage Score -202.35\n",
            "Episode 41\tAverage Score -202.36\n",
            "Episode 42\tAverage Score -202.31\n",
            "Episode 43\tAverage Score -202.31\n",
            "Episode 44\tAverage Score -202.31\n",
            "Episode 45\tAverage Score -202.34\n",
            "Episode 46\tAverage Score -202.34\n",
            "Episode 47\tAverage Score -202.32\n",
            "Episode 48\tAverage Score -202.34\n",
            "Episode 49\tAverage Score -202.32\n",
            "Episode 50\tAverage Score -202.31\n",
            "Episode 51\tAverage Score -202.35\n",
            "Episode 52\tAverage Score -202.35\n",
            "Episode 53\tAverage Score -202.32\n",
            "Episode 54\tAverage Score -202.32\n",
            "Episode 55\tAverage Score -202.34\n",
            "Episode 56\tAverage Score -202.35\n",
            "Episode 57\tAverage Score -202.36\n",
            "Episode 58\tAverage Score -202.38\n",
            "Episode 59\tAverage Score -202.40\n",
            "Episode 60\tAverage Score -202.37\n",
            "Episode 61\tAverage Score -202.38\n",
            "Episode 62\tAverage Score -202.34\n",
            "Episode 63\tAverage Score -202.32\n",
            "Episode 64\tAverage Score -202.33\n",
            "Episode 65\tAverage Score -202.35\n",
            "Episode 66\tAverage Score -202.35\n",
            "Episode 67\tAverage Score -202.35\n",
            "Episode 68\tAverage Score -202.36\n",
            "Episode 69\tAverage Score -202.38\n",
            "Episode 70\tAverage Score -202.37\n",
            "Episode 71\tAverage Score -202.38\n",
            "Episode 72\tAverage Score -202.38\n",
            "Episode 73\tAverage Score -202.39\n",
            "Episode 74\tAverage Score -202.39\n",
            "Episode 75\tAverage Score -202.40\n",
            "Episode 76\tAverage Score -202.42\n",
            "Episode 77\tAverage Score -202.40\n",
            "Episode 78\tAverage Score -202.41\n",
            "Episode 79\tAverage Score -202.40\n",
            "Episode 80\tAverage Score -202.39\n",
            "Episode 81\tAverage Score -202.40\n",
            "Episode 82\tAverage Score -202.41\n",
            "Episode 83\tAverage Score -202.42\n",
            "Episode 84\tAverage Score -202.41\n",
            "Episode 85\tAverage Score -202.40\n",
            "Episode 86\tAverage Score -202.42\n",
            "Episode 87\tAverage Score -202.42\n",
            "Episode 88\tAverage Score -202.42\n",
            "Episode 89\tAverage Score -202.43\n",
            "Episode 90\tAverage Score -202.44\n",
            "Episode 91\tAverage Score -202.45\n",
            "Episode 92\tAverage Score -202.45\n",
            "Episode 93\tAverage Score -202.45\n",
            "Episode 94\tAverage Score -202.45\n",
            "Episode 95\tAverage Score -202.45\n",
            "Episode 96\tAverage Score -202.44\n",
            "Episode 97\tAverage Score -202.44\n",
            "Episode 98\tAverage Score -202.43\n",
            "Episode 99\tAverage Score -202.44\n",
            "Episode 100\tAverage Score -202.44\n",
            "Episode 101\tAverage Score -202.43\n",
            "Episode 102\tAverage Score -202.43\n",
            "Episode 103\tAverage Score -202.42\n",
            "Episode 104\tAverage Score -202.41\n",
            "Episode 105\tAverage Score -202.41\n",
            "Episode 106\tAverage Score -202.41\n",
            "Episode 107\tAverage Score -202.41\n",
            "Episode 108\tAverage Score -202.41\n",
            "Episode 109\tAverage Score -202.41\n",
            "Episode 110\tAverage Score -202.40\n",
            "Episode 111\tAverage Score -202.40\n",
            "Episode 112\tAverage Score -202.40\n",
            "Episode 113\tAverage Score -202.40\n",
            "Episode 114\tAverage Score -202.40\n",
            "Episode 115\tAverage Score -202.39\n",
            "Episode 116\tAverage Score -202.39\n",
            "Episode 117\tAverage Score -202.39\n",
            "Episode 118\tAverage Score -202.40\n",
            "Episode 119\tAverage Score -202.39\n",
            "Episode 120\tAverage Score -202.39\n",
            "Episode 121\tAverage Score -202.40\n",
            "Episode 122\tAverage Score -202.40\n",
            "Episode 123\tAverage Score -202.41\n",
            "Episode 124\tAverage Score -202.41\n",
            "Episode 125\tAverage Score -202.41\n",
            "Episode 126\tAverage Score -202.41\n",
            "Episode 127\tAverage Score -202.43\n",
            "Episode 128\tAverage Score -202.42\n",
            "Episode 129\tAverage Score -202.44\n",
            "Episode 130\tAverage Score -202.44\n",
            "Episode 131\tAverage Score -202.44\n",
            "Episode 132\tAverage Score -202.43\n",
            "Episode 133\tAverage Score -202.44\n",
            "Episode 134\tAverage Score -202.43\n",
            "Episode 135\tAverage Score -202.43\n",
            "Episode 136\tAverage Score -202.43\n",
            "Episode 137\tAverage Score -202.42\n",
            "Episode 138\tAverage Score -202.42\n",
            "Episode 139\tAverage Score -202.43\n",
            "Episode 140\tAverage Score -202.43\n",
            "Episode 141\tAverage Score -202.43\n",
            "Episode 142\tAverage Score -202.43\n",
            "Episode 143\tAverage Score -202.42\n",
            "Episode 144\tAverage Score -202.41\n",
            "Episode 145\tAverage Score -202.41\n",
            "Episode 146\tAverage Score -202.40\n",
            "Episode 147\tAverage Score -202.40\n",
            "Episode 148\tAverage Score -202.40\n",
            "Episode 149\tAverage Score -202.40\n",
            "Episode 150\tAverage Score -202.41\n",
            "Episode 151\tAverage Score -202.41\n",
            "Episode 152\tAverage Score -202.41\n",
            "Episode 153\tAverage Score -202.41\n",
            "Episode 154\tAverage Score -202.42\n",
            "Episode 155\tAverage Score -202.42\n",
            "Episode 156\tAverage Score -202.42\n",
            "Episode 157\tAverage Score -202.42\n",
            "Episode 158\tAverage Score -202.42\n",
            "Episode 159\tAverage Score -202.42\n",
            "Episode 160\tAverage Score -202.42\n",
            "Episode 161\tAverage Score -202.42\n",
            "Episode 162\tAverage Score -202.41\n",
            "Episode 163\tAverage Score -202.41\n",
            "Episode 164\tAverage Score -202.42\n",
            "Episode 165\tAverage Score -202.42\n",
            "Episode 166\tAverage Score -202.42\n",
            "Episode 167\tAverage Score -202.41\n",
            "Episode 168\tAverage Score -202.40\n",
            "Episode 169\tAverage Score -202.40\n",
            "Episode 170\tAverage Score -202.39\n",
            "Episode 171\tAverage Score -202.38\n",
            "Episode 172\tAverage Score -202.39\n",
            "Episode 173\tAverage Score -202.40\n",
            "Episode 174\tAverage Score -202.41\n",
            "Episode 175\tAverage Score -202.40\n",
            "Episode 176\tAverage Score -202.40\n",
            "Episode 177\tAverage Score -202.40\n",
            "Episode 178\tAverage Score -202.40\n",
            "Episode 179\tAverage Score -202.41\n",
            "Episode 180\tAverage Score -202.41\n",
            "Episode 181\tAverage Score -202.41\n",
            "Episode 182\tAverage Score -202.41\n",
            "Episode 183\tAverage Score -202.41\n",
            "Episode 184\tAverage Score -202.41\n",
            "Episode 185\tAverage Score -202.41\n",
            "Episode 186\tAverage Score -202.41\n",
            "Episode 187\tAverage Score -202.41\n",
            "Episode 188\tAverage Score -202.41\n",
            "Episode 189\tAverage Score -202.40\n",
            "Episode 190\tAverage Score -202.39\n",
            "Episode 191\tAverage Score -202.38\n",
            "Episode 192\tAverage Score -202.39\n",
            "Episode 193\tAverage Score -202.38\n",
            "Episode 194\tAverage Score -202.39\n",
            "Episode 195\tAverage Score -202.39\n",
            "Episode 196\tAverage Score -202.39\n",
            "Episode 197\tAverage Score -202.40\n",
            "Episode 198\tAverage Score -202.39\n",
            "Episode 199\tAverage Score -202.40\n",
            "Episode 200\tAverage Score -202.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Record the Agent picking only what it thinks is the best action.\n",
        "\"\"\"\n",
        "\n",
        "video_recorder = RecordVideo(env, './naive_video')\n",
        "\n",
        "state = env.reset()\n",
        "video_recorder.start_video_recorder()\n",
        "\n",
        "STEPS = 1000\n",
        "for _ in range(STEPS):  \n",
        "\n",
        "  video_recorder.reset() # reset the environment and see the initial observation    \n",
        "\n",
        "  # Sample a random action from the entire action space\n",
        "  random_action = agent.act(state, eps = 1)\n",
        "\n",
        "  # # Take the action and get the new observation space.\n",
        "  new_obs, reward, done, info = env.step(random_action)\n",
        "\n",
        "  video_recorder.render() # Renders and records the environment.\n",
        "\n",
        "\n",
        "video_recorder.close_video_recorder()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3XkOjdekTMi",
        "outputId": "b56b624c-f467-44c3-be0d-0a2bb4b6f46f"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/record_video.py:79: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/naive_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  f\"Overwriting existing videos at {self.video_folder} folder \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plotting the scores.\n",
        "\"\"\"\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(episode_scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Step #')\n",
        "plt.show()\n",
        "\n",
        "# The score decreased. Wow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "P0dzzqAuWKNz",
        "outputId": "40753f79-cc72-425a-c911-90191b0386f3"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}